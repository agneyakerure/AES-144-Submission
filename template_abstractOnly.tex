% Template for 61st conference for non-peer-reviewed articled
\documentclass[convention]{aesconf}

% Graphics path
\graphicspath{{./}{figures/}}

% UTF-8 encoding is recommended but use one that works for you.
\usepackage[utf8]{inputenc}

% Highly recommended package for better looking text automatically.
\usepackage{microtype}

% Natbib is used for more control on citations. You can use other moderd
% bibliography packages but please try to match the provided style.
\usepackage[numbers,square]{natbib} 


% These are useful for different purposes.
\usepackage{color}
\usepackage{url}


% The full title of the paper
\title{Audio Source Localization as an Input to Virtual Reality Systems}

% Put the authors in order here. The number in brackets define the corresponding affiliation.
\author[1]{Agneya A. Kerure}
\author[1]{Jason Freeman}

% Affiliations go here
\affil[1]{Georgia Institute of Technology}

% Correspondece should include the corresponding author's name and e-mail address
\correspondence{Agneya A. Kerure}{kerure.agneya@gatech.edu}

% These are used for headers. Anything that fits is okay. Please use proper punctuation.

% If there are many authors, please use the form "First author et al."
\lastnames{Kerure and Freeman}

% Short title should describe your topic but not be too long.
\shorttitle{Audio Source Localization in VR}


% This is required and draws the top title
\include{aestitle}

\begin{document}


\twocolumn[
\maketitle % MANDATORY! 

\begin{onecolabstract}
This paper details an effort towards incorporating audio source localization as an input to virtual reality systems, focusing primarily on games. The goal of this research is to find a novel method to use live audio as an input for level generation or creation of elements and objects in a virtual reality environment. The paper discusses the current state of audio-based games and virtual reality, and details the design requirements of a system consisting of a circular microphone array which can be used to localize the input audio. The paper also briefly discusses signal processing techniques used for audio information retrieval, and introduces a prototype of an asymmetric virtual reality first-person shooter game as proof-of-concept of the potential of audio source localization for augmenting the immersive nature of virtual reality.
\end{onecolabstract}
]

\section{Introduction}
Audio and music have historically been a passive part of video games. Soundtracks that accompany video games are used to captivate the player's attention and incorporate a sense of realism to the game environment, but they have rarely been an active part of the game-play mechanism until recently.
Music driven games started out in the form of rhythm based games where the player's sense of rhythm is challenged. These games typically involve a player in performing certain actions in the form of a dance or pressing buttons in a sequence. With the advent of virtual reality, 
Virtual reality rhythm games mostly work in a similar manner but rely on hand held controllers or motion trackers to interact with the rhythmic elements of the game.
Apart from rhythm, there are a few games which use audio from the user as a core mechanism of game-play. These typically rely on features like volume and pitch to control some aspects of the game.
Speech recognition has also been used as an input modality, typically for user interfaces in games.
The use of audio and its properties as an input to games has been very minimal and with the advancement in gaming technology and signal processing prowess of computers, exploration of these possible real-time interactions is warranted.
An important question to consider would be why use audio at all. There are other ways to locate objects and people in a vicinity around a person which include computer vision, infrared tracking, and even GPS. The advantage audio offers is the ability to process the received signal and get vast amounts of information from it, such as timbral and temporal properties, which can be used in different ways in the game with minimal hardware costs.
Virtual reality offers a paradigm which enables interactions in 360$^\circ$. To fully utilize this paradigm, the system involves a circular microphone array which localizes sounds in 360$^\circ$. This enables truly immersive interactions.
 

\section{Background}
Rhythm based games started out in Japan as electromechanical arcade games where users would press buttons in rhythm to score more points. As time progresses, the required rhythm gets more complicated and increases in speed. These games would also involve more than one player taking turns and pressing buttons.

Mobile-based games have also used audio as an input to control certain aspects of the game. Chicken Scream \footnote{http://www.perfecttapgames.com} is a mobile game in which the user controls the main character by shouting and screaming. The louder the shout, the faster the character moves. Modulation of volume is used as a mechanism to overcome in-game obstacles.

Audio-only games refer to games which can only be played and perceived through sound and acoustics. They were initially developed for the visually impaired community but have huge potential for mobile and multi-player gaming \cite{rober2005playing}. Although these games are limited in the amount of information which can be conveyed to the player, they also have a few advantages over conventional games, such as increased degree of spatial freedom, no necessity of screens and other costly equipment depending upon the game, lower computational complexity and hence less latency, making these games perfectly suited for mobile gaming.

There are many games in the virtual reality category which use music as an input to generate in-game content. Audioshield\footnote{http://audio-shield.com/} and Rockband VR\footnote{http://www.rockbandvr.com/} are the most important examples of this genre. These typicaly use predetermined times which are determined the  music being played to create cues in the forms of game-objects with which the user interacts.

/////////////////Should I add more research here? \cite{Igarashi:2001:VSU:502348.502372}

\section{System}  
This section describes the signal processing algorithms used, and the hardware and software requirements of the system.

- brief overview of how we localize sounds and localization systems/techniques

- how circular microphone array does it

- Pitch Detection
For pitch detection, the system uses the time-domain based autocorrelation approach. In this method, the autocorrelation sequence of each frame of the input audio is determined and the time lag corresponding to the second-largest peak from the central peak is used to determine the fundamental pitch period. This technique is most efficient in the mid- to low-frequency range and thus has been popular in speech recognition applications. The autocorrelation function is represented by: 

\[ [R] (t)  = \int_{i=-{\infty}}^{\infty} S_i^*(\tau) S_i(t+\tau) \delta \tau  \]

- Frequency band detection 

\subsection{Tools} 
/////////////////Is this section even important??

The author proposes a system which includes a microphone array for the capture of audio to estimate the incident audio angle for the HTC Vive Virtual Reality headset which allows users to move in a restricted 3-D space and interact with it using hand-held controllers which are motion tracked. 

The system uses a ReSpeaker 7-channel Circular Microphone Array\footnote{https://www.seeedstudio.com/} which features an XMOS XVSM-2000 DSP chip, providing excellent performance for word detection. It has a small form factor along with an LED ring to show the direction of arrival it detects. 

The development of the game has been done using Unity3D as the main IDE. The system described in this paper uses JUCE \footnote{https://juce.com} to process the input audio from the microphones. This is because Unity does not natively support stereo audio input for processing. The calculated lag values from JUCE are sent to Unity using Open Sound Control(OSC), which is a communication protocol optimized for modern networking technology. Both JUCE and Unity natively support OSC communication.

The speech recognition used in the system is accomplished using the Microsoft Speech Recognition tool for Unity. 

\section{Game Design} 

\subsection{Mechanics and UI} 

\section{Feedback} 
The game has undergone several iterations of prototyping based on user feedback. Most of the feedback was positive, with people getting excited about the potential of this technology in the world of interactive games and software. There are many robust algorithms which utilize microphone arrays to localize more than one sound source at a time. The test users were excited to learn about the possibilities these sophisticated approaches may provide in the realm of virtual reality experiences.

As for the design of the game, it was noticed that there was  a visible imbalance between the in-game skills required by the VR player and the non-VR player who uses his voice. There was very little skill required by the non-VR player to win the game as he could just keep saying the keyword to generate a large number of projectiles towards the VR player. This helped realize the need to limit the number of projectiles created. There was also a need to restrict potential projectile spawn points so that it is easier for the players to understand the game and how movement was actually correlated to the spawning of the projectiles. 
To increase the skill required and the involvement of audio localization, the non-VR player was given the ability to control the path of the projectile towards the VR player by singing and changing the pitch of his voice - the pitch was directly correlated to the y-axis displacement from the original trajectory. This also provided for a fun experience for the VR user as he now had to try and guess where the opponent would take his projectile and shoot at it. 

An important point of discussion was the use of a real-time key word detection library over a trained classifier, which provided much less latency but also compromised on accuracy. Because of the limited keywords used, it was deemed that a classifier would serve the needs for this game better but due to limitations in time, the author was not able to implement one. The Microsoft keyword detection software, however, does the job fairly decently with acceptable latency.

Another important point of discussion was the decision to use keyword detection over singing voice analysis for the generation of game objects. This is a variation in the type of game-play made possible by the technology. The game in its current form tries to incorporate both keyword detection and singing voice analysis by utilizing pitch detection to move the instantiated projectiles. More complicated and robust singing voice analysis will be utilized in upcoming iterations of the game.

There was an understandable bias towards wanting to be the VR player rather than the one trying to attack using the microphones due to the truly immersible nature of virtual reality. Future enhancements to the audio localization system will attempt to bridge this preference gap by increasing the functionality and intuitive ease of use for the non-VR player.

\section{Challenges}
There were a few main challenges which were addressed in the different stages of development of the system: 
\begin{itemize}
\item Unity not supporting multi channel audio from audio interfaces natively made the use of an external audio manager a necessity. The JUCE C++ library was chosen for this task because of its robust audio handling capabilities.
\item Real-time audio source localization, information extraction, and keyword detection have varying amounts of latency associated with them. Meaningfully mapping this information to instantiated game objects so that it is understood by the players is a challenge.
\item The restrictions introduced on the in-game audio by the acoustic nature of the system present a huge trade-off.
\end{itemize}

\section{Future Work} 

Summarize your work and conclude.

\bibliographystyle{jaes}

% Reference to bibliography file.
\bibliography{refs}


\end{document}